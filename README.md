# PDEnet_GPU  
read me  
前情题要：因为本人代码能力有限，不排除这个代码存在各种各样的bug。但是
至少运行我写出来的这些代码是可以跑通的（当然在我的电脑测试过）。如果
大家有问题或者对代码有更好的想法都可以在issue里面提，我有时间会看或者
回复交流。   
代码注释都是我自己写的，对代码功能的解析也都是我写的，所以有问题甚至错误都在所难免，希望海涵！如果看到错误也可以在issue里提醒一下后人
（如果问题特别严重我可能也会修改一下，小错误因时间原因就不改了，望海涵）。
祝大家project顺利。


文件相关：

1.环境配置
python>=3.6 如果装了cuda，pytorch和cuda版本要匹配（不然可能运行不了gpu版本）


2.代码说明：
 
 2.1调整训练epoch数  
 在Option中eopchs_Adam 和 epochs_LBFGS 设置
因为我运行的时候设置的时100 和 20 （老师是5000和200）运行次数很少，所以测试效果
没有老师的好，可以更改这两个参数来提高精度。
 
 2.2代码默认跑GPU（测试集是cpu）如何切换成cpu？  
Option的parse方法中将我注释的那行取消掉，并且在训练的那个单元中
将#args.problem = Problem(env = 'cpu')这个注释的内容代替原来
的problem声明命令

 2.3为什么老师的代码gpu会报错  
因为有些参与运算的参数并没有被传进gpu里面，这些参数的一部分可以在
Trainer中定义，还有一部分只能在Problem中定义，所以我在Problem中
加了一个参数，‘env’，默认是GPU，也可以规定是cpu

注意！因为我只有训练集的参数设置了可以传进gpu，测试器是没有这个设置的。也就是当你用Tester画图
的时候务必要用cpu类型的Problem，不然会报错。

 2.4为什么gpu版本运行速度和cpu版本没有提升，甚至还有些降低？  
在运行的时候你可以顺便查看一下资源管理器中的gpu利用率，发现非常低。
也就是在运行的时候大部分时间gpu是处于空闲状态的。为什么会这样？
因为这个网络模型太小了，计算的数据每个epoch也太少。导致程序运行的
大部分时间都花在gpu，cpu和内存的i/o交换上。如果你仔细看我的代码，你会发现
每次从Problem生成的参数也要放进gpu里，而在一般（如图像处理）的网络中
是不存在这种操作的。

 2.5有没有什么办法可以提升gpu的利用率。  
更好的代码结构可能可以做到，不过我比较菜。如果有大神做到了也可以让我
膜拜一下。
